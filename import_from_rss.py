import os
import re
import json
from datetime import datetime
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

# ===== í™˜ê²½ ë³€ìˆ˜(.env) ë¡œë“œ & OpenAI í´ë¼ì´ì–¸íŠ¸ =====
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ===== ì„¤ì • =====
# ì˜ˆ: https://your-wp-site.com/wp-json/wp/v2/posts
WP_API_BASE = os.getenv("WP_API_BASE")

CONTENT_BASE = "content/news"
IMAGE_BASE = "static/images/news"

DEFAULT_CATEGORY = "ë¸”ë¡ì²´ì¸"      # ì¹´í…Œê³ ë¦¬ ê³ ì •
TIME_SUFFIX = "T09:00:00+09:00"    # í•œêµ­ ì‹œê°„ ê¸°ì¤€ ê³ ì •

MAX_POSTS = 100                    # ìµœëŒ€ ê°€ì ¸ì˜¬ í¬ìŠ¤íŠ¸ ìˆ˜
PER_PAGE = 50                      # WP API per_page (ìµœëŒ€ 100)
# =========================


def slugify(text: str) -> str:
    """ì œëª© ê¸°ë°˜ slug ìƒì„± (í•œê¸€+ì˜ë¬¸+ìˆ«ìë§Œ, ê³µë°±ì€ -)"""
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9ê°€-í£\- ]", "", text)
    text = text.replace(" ", "-")
    return text[:60]


def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)


def clean_html_to_markdown(html: str) -> str:
    """ë³¸ë¬¸ HTML â†’ ìµœì†Œí•œì˜ ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬"""
    soup = BeautifulSoup(html, "html.parser")

    # br/hr ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ
    for br in soup.find_all(["br", "hr"]):
        br.replace_with("\n")

    text = soup.get_text("\n")
    lines = [line.strip() for line in text.splitlines()]
    lines = [line for line in lines if line]
    return "\n\n".join(lines)


def extract_first_image_from_html(
    html: str, base_url: str | None = None
) -> str | None:
    """content.rendered ì•ˆì— <img>ê°€ ìˆì„ ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ src ë°˜í™˜"""
    soup = BeautifulSoup(html, "html.parser")
    img = soup.find("img")
    if img and img.get("src"):
        src = img["src"]
        if base_url and (src.startswith("/") or not src.startswith("http")):
            return urljoin(base_url, src)
        return src
    return None


def extract_featured_image_from_post(
    post: dict, content_html: str, base_url: str | None = None
) -> str | None:
    """
    ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ:
    1ìˆœìœ„: REST APIì˜ _embedded.wp:featuredmedia.source_url
    2ìˆœìœ„: content.rendered ì•ˆì˜ ì²« ë²ˆì§¸ <img>
    """
    try:
        embedded = post.get("_embedded", {})
        media_list = embedded.get("wp:featuredmedia")
        if isinstance(media_list, list) and media_list:
            media = media_list[0]
            url = media.get("source_url")
            if not url:
                url = (
                    media.get("media_details", {})
                    .get("sizes", {})
                    .get("full", {})
                    .get("source_url")
                )
            if url:
                return url
    except Exception:
        pass

    # fallback: content ì•ˆì—ì„œ <img> ì°¾ê¸°
    return extract_first_image_from_html(content_html, base_url)


def rewrite_with_openai(title: str, content: str) -> tuple[str, str, str, list[str]]:
    """
    ì˜ì–´ ì›Œë“œí”„ë ˆìŠ¤ ê¸€ì„:
    - í•œêµ­ì–´ ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤íƒ€ì¼ë¡œ ì¬ì‘ì„±
    - í´ë¦­ ì˜ ë‚˜ì˜¤ëŠ” ìƒˆ ì œëª©
    - í•œ ì¤„ ìš”ì•½(summary)
    - í•œêµ­ì–´ íƒœê·¸ ë¦¬ìŠ¤íŠ¸
    ë¥¼ ìƒì„±í•´ì„œ (ìƒˆ ì œëª©, ìš”ì•½, ìƒˆ ë³¸ë¬¸, íƒœê·¸ëª©ë¡)ì„ ë°˜í™˜
    """
    prompt = f"""
ë„ˆëŠ” ë¸”ë¡ì²´ì¸Â·ê°€ìƒìì‚° ë‰´ìŠ¤ë¥¼ ë‹¤ë£¨ëŠ” í•œêµ­ì–´ ì˜¨ë¼ì¸ ë¯¸ë””ì–´ì˜ í¸ì§‘ ê¸°ìë‹¤.
ì•„ë˜ ì˜ì–´ ì›ë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ, í•œêµ­ ë…ìë¥¼ ìœ„í•œ ê¸°ì‚¬ë¡œ ì¬êµ¬ì„±í•´ì¤˜.

[ì›ë˜ ì œëª©]
{title}

[ì›ë˜ ë³¸ë¬¸]
{content}

ìš”êµ¬ì‚¬í•­:
- ê²°ê³¼ë¬¼ì€ **ë°˜ë“œì‹œ í•œêµ­ì–´**ë¡œ ì‘ì„±í•  ê²ƒ
- ì œëª©(title):
  - í´ë¦­ë¥ (CTR)ì´ ë†’ê²Œ ë³´ì´ë„ë¡ ìƒˆë¡­ê²Œ ì¬ì°½ì‘
  - ì›ë˜ ì œëª©ì„ ê·¸ëŒ€ë¡œ ë²ˆì—­í•˜ê±°ë‚˜ ë³µì‚¬í•˜ì§€ ë§ ê²ƒ
- ìš”ì•½(summary):
  - 1~2ë¬¸ì¥, 120ì ë‚´ì™¸
  - ê¸°ì‚¬ì˜ í•µì‹¬ í¬ì¸íŠ¸(ê°€ê²© ë³€ë™, ì£¼ìš” ë°œì–¸, ê·œì œ ì´ìŠˆ ë“±)ë¥¼ ê°„ë‹¨íˆ ì •ë¦¬
- íƒœê·¸(tags):
  - í•œêµ­ì–´ ë‹¨ì–´/êµ¬ë¡œë§Œ êµ¬ì„±
  - ì˜ˆ: ["ë¹„íŠ¸ì½”ì¸", "ì´ë”ë¦¬ì›€", "í˜„ë¬¼ ETF", "SEC", "ì˜¨ì²´ì¸ ë°ì´í„°"]
  - 3~7ê°œ ì •ë„, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ
- ë³¸ë¬¸(content):
  - ë¸”ë¡œê·¸ìš© ë‰´ìŠ¤ ê¸°ì‚¬ í†¤ (ë„ˆë¬´ ìºì£¼ì–¼ X, ë„ˆë¬´ ë…¼ë¬¸ì²´ X)
  - ì›ë¬¸ì´ ë‹´ê³  ìˆëŠ” ì‚¬ì‹¤ ê´€ê³„, ìˆ˜ì¹˜(ê°€ê²©, ë‚ ì§œ, ìˆ˜ëŸ‰ ë“±)ëŠ” ì •í™•íˆ ìœ ì§€
  - ë¶ˆí•„ìš”í•œ ë°˜ë³µ/êµ°ë”ë”ê¸° ë¬¸ì¥ì€ ì •ë¦¬
  - ë‹¨ë½ì„ ì ì ˆíˆ ë‚˜ëˆ ì„œ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±

ë°˜í™˜ í˜•ì‹(JSON) ì˜ˆì‹œ:
{{
  "title": "ìƒˆë¡œ ì¬ì‘ì„±ëœ í•œêµ­ì–´ ì œëª©",
  "summary": "ê¸°ì‚¬ë¥¼ 1~2ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•œ í•œêµ­ì–´ ë¬¸ì¥.",
  "tags": ["ë¹„íŠ¸ì½”ì¸", "ETF", "SEC"],
  "content": "ì¬ì‘ì„±ëœ í•œêµ­ì–´ ë³¸ë¬¸ ì „ì²´"
}}

JSONë§Œ ì¶œë ¥í•´ì¤˜.
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
        )

        msg = resp.choices[0].message

        # SDK ë²„ì „ì— ë”°ë¼ content íƒ€ì…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ë°©ì–´ì ìœ¼ë¡œ ì²˜ë¦¬
        if isinstance(msg.content, list):
            content_str = "".join(
                getattr(part, "text", str(part)) for part in msg.content
            )
        else:
            content_str = msg.content

        data = json.loads(content_str)

        new_title = data.get("title", title).strip()
        new_summary = data.get("summary", "").strip()
        new_content = data.get("content", content).strip()

        raw_tags = data.get("tags", [])
        if isinstance(raw_tags, list):
            new_tags = [str(t).strip() for t in raw_tags if str(t).strip()]
        else:
            # "ë¹„íŠ¸ì½”ì¸, ETF, SEC" ì´ëŸ° ì‹ìœ¼ë¡œ ì˜¬ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë¶„ë¦¬
            new_tags = [t.strip() for t in str(raw_tags).split(",") if t.strip()]

        # ëª¨ë¸ì´ ì›ì œëª© ê·¸ëŒ€ë¡œ ëŒë ¤ì¤„ ê²½ìš° ìµœì†Œí•œì˜ ë³€í˜•
        if new_title == title:
            new_title = f"{title}â€¦ í•µì‹¬ ì´ìŠˆ ì •ë¦¬"

        return new_title, new_summary, new_content, new_tags

    except Exception as e:
        print("[WARN] OpenAI ì¬ì‘ì„± ì‹¤íŒ¨:", e)
        # ì‹¤íŒ¨ ì‹œ: ì›ë³¸ ê¸°ì¤€ìœ¼ë¡œ fallback
        fallback_summary = (
            (content[:150].replace("\n", " ") + "â€¦") if content else ""
        )
        return title, fallback_summary, content, []


def fetch_wp_posts(
    max_posts: int = MAX_POSTS, per_page: int = PER_PAGE
) -> list[dict]:
    """
    WP REST APIì—ì„œ posts JSONì„ ìµœëŒ€ max_postsê¹Œì§€ ê°€ì ¸ì˜¨ë‹¤.
    _embed=1ì„ ë¶™ì—¬ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ ì •ë³´ê¹Œì§€ ê°€ì ¸ì˜¨ë‹¤.
    """
    collected: list[dict] = []
    page = 1

    if not WP_API_BASE:
        raise RuntimeError("WP_API_BASE í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    while len(collected) < max_posts:
        params = {"per_page": per_page, "page": page, "_embed": "1"}
        print(f"[INFO] WP posts ìš”ì²­: page={page}, per_page={per_page}")
        resp = requests.get(WP_API_BASE, params=params, timeout=10)

        if resp.status_code != 200:
            print(f"[WARN] WP API ìš”ì²­ ì‹¤íŒ¨ status={resp.status_code}")
            break

        items = resp.json()
        if not items:
            print("[INFO] ë” ì´ìƒ ê°€ì ¸ì˜¬ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break

        collected.extend(items)
        if len(items) < per_page:
            break

        page += 1

    return collected[:max_posts]


def main():
    print(f"[INFO] WP JSONì—ì„œ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {WP_API_BASE}")
    posts = fetch_wp_posts(MAX_POSTS, PER_PAGE)
    print(f"[INFO] ì´ ê°€ì ¸ì˜¨ í¬ìŠ¤íŠ¸ ìˆ˜: {len(posts)}")

    for post in posts:
        # ì› ì œëª©
        raw_title = post.get("title", {}).get("rendered", "") or "ì œëª© ì—†ìŒ"
        orig_title = (
            BeautifulSoup(raw_title, "html.parser").get_text().strip()
        )

        # ë§í¬ (ì´ë¯¸ì§€ ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°ì—ë§Œ ì‚¬ìš©)
        link = post.get("link", "").strip()

        # ë‚ ì§œ
        raw_date = post.get("date") or post.get("date_gmt") or ""
        try:
            dt = datetime.fromisoformat(raw_date.replace("Z", ""))
        except Exception:
            dt = datetime.now()

        date_str = dt.strftime("%Y-%m-%d")
        year = dt.strftime("%Y")
        month = dt.strftime("%m")

        # slug (ì›ë˜ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ë§Œë“œëŠ” ê²Œ ì•ˆì „)
        slug_base = slugify(orig_title) or "untitled"
        slug = f"{date_str}-{slug_base}"

        # ê²½ë¡œ
        content_dir = os.path.join(CONTENT_BASE, year, month)
        ensure_dir(content_dir)
        md_path = os.path.join(content_dir, f"{slug}.md")

        if os.path.exists(md_path):
            print(f"[SKIP] ì´ë¯¸ ì¡´ì¬: {md_path}")
            continue

        # ë³¸ë¬¸ HTML
        raw_content_html = (
            post.get("content", {}).get("rendered", "")
            or post.get("excerpt", {}).get("rendered", "")
            or ""
        )

        body_text_raw = clean_html_to_markdown(raw_content_html)

        # ğŸ”¹ OpenAIë¡œ ì œëª©+ë³¸ë¬¸ ì¬ì‘ì„± (í•œêµ­ì–´ ê¸°ì‚¬ + ìš”ì•½ + íƒœê·¸)
        new_title, new_summary, new_body, new_tags = rewrite_with_openai(
            orig_title, body_text_raw
        )
        title = new_title
        summary_text = new_summary
        body_text = new_body
        tags = new_tags
        print(f"[AI] ì œëª© ì¬ì‘ì„±: '{orig_title}'  â†’  '{title}'")

        # ğŸ”¹ ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ (REST API + fallback)
        img_url = extract_featured_image_from_post(
            post, raw_content_html, base_url=link
        )
        featured_image = ""

        if img_url:
            try:
                parsed = urlparse(img_url)
                ext = os.path.splitext(parsed.path)[1]
                if ext.lower() not in [".jpg", ".jpeg", ".png", ".webp"]:
                    ext = ".jpg"

                img_dir = os.path.join(IMAGE_BASE, year, month)
                ensure_dir(img_dir)
                img_filename = slug + ext
                img_path = os.path.join(img_dir, img_filename)

                print(f"[IMG] ë‹¤ìš´ë¡œë“œ: {img_url} -> {img_path}")

                headers = {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    )
                }
                r = requests.get(img_url, headers=headers, timeout=10)
                print(f"[IMG] status={r.status_code}")

                if r.status_code == 200:
                    with open(img_path, "wb") as f:
                        f.write(r.content)
                    # XMag ë¦¬ìŠ¤íŠ¸ìš©: thumbnail: "images/news/YYYY/MM/íŒŒì¼ëª…"
                    featured_image = f"news/{year}/{month}/{img_filename}"
                else:
                    print(
                        f"[WARN] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ status={r.status_code}"
                    )
            except Exception as e:
                print(f"[WARN] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # ===== front matter ì‘ì„± =====
        safe_title = title.replace('"', '\\"')
        safe_summary = (summary_text or "").replace('"', '\\"')

        front_matter = "---\n"
        front_matter += f'title: "{safe_title}"\n'
        front_matter += f"date: {date_str}{TIME_SUFFIX}\n"
        front_matter += f"lastmod: {date_str}{TIME_SUFFIX}\n"
        front_matter += "draft: false\n"
        front_matter += f'categories: ["{DEFAULT_CATEGORY}"]\n'

        # ğŸ”¹ íƒœê·¸ ì±„ìš°ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        front_matter += "tags:\n"
        for t in (tags or []):
            safe_tag = str(t).replace('"', '\\"')
            front_matter += f'  - "{safe_tag}"\n'

        front_matter += f'summary: "{safe_summary}"\n'

        if featured_image:
            # XMag list.htmlì—ì„œ .Params.thumbnail ì„ ë³´ê³  ìˆìœ¼ë¯€ë¡œ thumbnail ì‚¬ìš©
            front_matter += f'thumbnail: "{featured_image}"\n'

        front_matter += "---\n\n"

        full_content = front_matter + body_text + "\n"

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(full_content)

        print(f"[OK] ìƒì„±: {md_path}")

    print("[DONE] WP JSON â†’ Hugo ë³€í™˜ ì™„ë£Œ")


if __name__ == "__main__":
    main()
