import os
import re
import json
from datetime import datetime
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI

# ===== í™˜ê²½ ë³€ìˆ˜(.env) ë¡œë“œ & OpenAI í´ë¼ì´ì–¸íŠ¸ =====
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ===== ì„¤ì • =====
# ì˜ˆ: https://your-wp-site.com/wp-json/wp/v2/posts
WP_API_BASE = os.getenv("WP_API_BASE")

CONTENT_BASE = "content/news"
IMAGE_BASE = "static/images/news"

DEFAULT_CATEGORY = "ë¸”ë¡ì²´ì¸"      # ì¹´í…Œê³ ë¦¬ ê³ ì •
TIME_SUFFIX = "T09:00:00+09:00"    # í•œêµ­ ì‹œê°„ ê¸°ì¤€ ê³ ì •

MAX_POSTS = 100                    # ìµœëŒ€ ê°€ì ¸ì˜¬ í¬ìŠ¤íŠ¸ ìˆ˜
PER_PAGE = 50                      # WP API per_page (ìµœëŒ€ 100)
# =========================


def slugify(text: str) -> str:
    """ì œëª© ê¸°ë°˜ slug ìƒì„± (í•œê¸€+ì˜ë¬¸+ìˆ«ìë§Œ, ê³µë°±ì€ -)"""
    text = text.strip().lower()
    text = re.sub(r"[^a-z0-9ê°€-í£\- ]", "", text)
    text = text.replace(" ", "-")
    return text[:60]


def ensure_dir(path: str):
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)


def clean_html_to_markdown(html: str) -> str:
    """ë³¸ë¬¸ HTML â†’ ìµœì†Œí•œì˜ ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬"""
    soup = BeautifulSoup(html, "html.parser")

    # br/hr ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ
    for br in soup.find_all(["br", "hr"]):
        br.replace_with("\n")

    text = soup.get_text("\n")
    lines = [line.strip() for line in text.splitlines()]
    lines = [line for line in lines if line]
    return "\n\n".join(lines)


def extract_first_image_from_html(
    html: str, base_url: str | None = None
) -> str | None:
    """content.rendered ì•ˆì— <img>ê°€ ìˆì„ ê²½ìš° ì²« ë²ˆì§¸ ì´ë¯¸ì§€ src ë°˜í™˜"""
    soup = BeautifulSoup(html, "html.parser")
    img = soup.find("img")
    if img and img.get("src"):
        src = img["src"]
        if base_url and (src.startswith("/") or not src.startswith("http")):
            return urljoin(base_url, src)
        return src
    return None


def extract_featured_image_from_post(
    post: dict, content_html: str, base_url: str | None = None
) -> str | None:
    """
    ëŒ€í‘œ ì´ë¯¸ì§€ URL ì¶”ì¶œ:
    1ìˆœìœ„: REST APIì˜ _embedded.wp:featuredmedia.source_url
    2ìˆœìœ„: content.rendered ì•ˆì˜ ì²« ë²ˆì§¸ <img>
    """
    try:
        embedded = post.get("_embedded", {})
        media_list = embedded.get("wp:featuredmedia")
        if isinstance(media_list, list) and media_list:
            media = media_list[0]
            url = media.get("source_url")
            if not url:
                url = (
                    media.get("media_details", {})
                    .get("sizes", {})
                    .get("full", {})
                    .get("source_url")
                )
            if url:
                return url
    except Exception:
        pass

    # fallback: content ì•ˆì—ì„œ <img> ì°¾ê¸°
    return extract_first_image_from_html(content_html, base_url)


def rewrite_with_openai(title: str, content: str) -> tuple[str, str, str, list[str]]:
    """
    ì°¸ê³  í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¸”ë¡œê·¸í˜• ì¹¼ëŸ¼(ì•½ 1000ì) ìƒì„±
    - title: ì œëª©(ì•ì— [ë°”ì´ë‚¸ìŠ¤] ìë™)
    - summary: 2~3ë¬¸ì¥ ìš”ì•½
    - body: ë³¸ë¬¸(ì•½ 900~1100ì)
    - tags: íƒœê·¸ ë¦¬ìŠ¤íŠ¸(ìµœëŒ€ 8ê°œ)
    """
    prompt = f"""
ë„ˆëŠ” 'ë¸”ë¡ì²´ì¸ ê³µë¶€ ê¸°ë¡ ë¸”ë¡œê·¸'ì˜ ê¸€ì“´ì´ì•¼.
ì•„ë˜ í…ìŠ¤íŠ¸ëŠ” ì°¸ê³ ìë£Œì¼ ë¿ì´ë©°, ì›ë¬¸ì„ ì¬ì‘ì„±/ì˜ì—­/ìš”ì•½í•´ì„œëŠ” ì•ˆ ë¼.
ë°˜ë“œì‹œ **ë‚´ê°€ ì§ì ‘ ìƒê°í•´ì„œ ì“´ ë¸”ë¡œê·¸ ê¸€**ì²˜ëŸ¼ ì™„ì „íˆ ìƒˆë¡œ ì‘ì„±í•´ì¤˜.

[ì°¸ê³  ì œëª©]
{title}

[ì°¸ê³  ë‚´ìš©]
{content}

âœ… ì‘ì„± ëª©í‘œ(ì•„ì£¼ ì¤‘ìš”)
- ê²°ê³¼ë¬¼ì€ "ë¸”ë¡œê·¸ ê¸€"ì´ë©°, ê¸°ì/ë‰´ìŠ¤/ë¦¬í¬íŠ¸ ë¬¸ì²´ ê¸ˆì§€
- 'ë°œí‘œí–ˆë‹¤/ì „ë§ëœë‹¤/ë°í˜”ë‹¤/ê´€ê³„ì' ê°™ì€ ë‰´ìŠ¤ í‘œí˜„ ê¸ˆì§€
- ë¬¸ì¥ êµ¬ì¡°ë¥¼ ì›ë¬¸ê³¼ ì™„ì „íˆ ë‹¤ë¥´ê²Œ(ì›ë¬¸ ë¬¸ì¥ ê·¸ëŒ€ë¡œ ì‚¬ìš© ê¸ˆì§€)
- ë‚´ê°€ ê³µë¶€í•˜ë©´ì„œ ë– ì˜¬ë¦° ì§ˆë¬¸/ìƒê°/ê´€ì ì„ 1ì¸ì¹­ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ê³¼ì¥, í™•ì •ì  ë‹¨ì •, ì„ ë™ì  í‘œí˜„ ê¸ˆì§€

âœ… ë¶„ëŸ‰
- ë³¸ë¬¸ì€ **ê³µë°± ì œì™¸ ì•½ 1000ì(900~1100ì)** ë²”ìœ„
- ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼

âœ… SEO ë¸”ë¡œê·¸ êµ¬ì¡°(ì´ ìˆœì„œ ê·¸ëŒ€ë¡œ)
1) ë„ì…(2~3ë¬¸ì¥): ë…ìê°€ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ì™”ì„ ë•Œ "ì™œ ì¤‘ìš”í•œì§€" ë°”ë¡œ ë‚©ë“ë˜ëŠ” í›„í‚¹
2) í•µì‹¬ ì •ë¦¬(3~5ë¬¸ì¥): ì°¸ê³  ë‚´ìš©ì˜ 'ë§¥ë½'ë§Œ ë‚´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…(ë³µë¶™ ê¸ˆì§€)
3) ë‚´ê°€ ë³´ëŠ” í¬ì¸íŠ¸(ë¶ˆë¦¿ 3ê°œ): "ë‚´ê°€ ì£¼ëª©í•œ ì "ì„ ì§§ê²Œ ìš”ì•½
4) ë‚´ ìƒê°/í•´ì„(6~9ë¬¸ì¥): ê³µë¶€í•˜ë©´ì„œ ë“  ìƒê°, í—·ê°ˆë¦° ì§€ì , ì—°ê²°ë˜ëŠ” ê°œë… ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ
5) ë§ˆë¬´ë¦¬(2~3ë¬¸ì¥): ë…ìì—ê²Œ ë˜ì§€ëŠ” ì§ˆë¬¸ 1ê°œ + ë‹¤ìŒì— ë” íŒŒë³¼ ì£¼ì œ 1ê°œ

âœ… í†¤ ê°€ì´ë“œ
- ë¸”ë¡œê·¸ ì¹¼ëŸ¼ í†¤: ì¹œì ˆí•˜ì§€ë§Œ ê°€ë³ì§€ ì•Šê²Œ
- "ì €ëŠ”/ì œê°€/ê°œì¸ì ìœ¼ë¡œ/ì œ ê¸°ì¤€ì—ì„œëŠ”" ê°™ì€ 1ì¸ì¹­ í‘œí˜„ì„ ìµœì†Œ 3ë²ˆ í¬í•¨
- ë¬¸ì¥ì— ë‚ ì§œ/ì†ë³´/ë‹¨ë… ê°™ì€ ë‰´ìŠ¤ ìš”ì†Œ ë„£ì§€ ë§ ê²ƒ

âœ… ì¶œë ¥(JSON) í˜•ì‹ (ë°˜ë“œì‹œ ì´ í‚¤ë§Œ ì‚¬ìš©)
{{
  "title": "ìƒˆ ì œëª©(í´ë¦­ ìœ ë„í˜•ì´ë˜ ê³¼ì¥ ê¸ˆì§€)",
  "summary": "2~3ë¬¸ì¥ ìš”ì•½(ë¸”ë¡œê·¸ ì†Œê°œê¸€ì²˜ëŸ¼)",
  "content": "ë³¸ë¬¸(ì•½ 1000ì)",
  "tags": ["ë¸”ë¡ì²´ì¸", "íƒœê·¸2", "íƒœê·¸3", "íƒœê·¸4", "íƒœê·¸5"]
}}

ì¶”ê°€ ê·œì¹™:
- titleì—ëŠ” [ë°”ì´ë‚¸ìŠ¤]ë¥¼ ë¶™ì´ì§€ ë§ˆ(ì½”ë“œì—ì„œ ë¶™ì„).
- tagsëŠ” ìµœëŒ€ 8ê°œ, ì¤‘ë³µ ê¸ˆì§€, ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ë§Œ ë‚˜ì—´í•˜ì§€ ë§ ê²ƒ.
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
        )

        msg = resp.choices[0].message
        content_str = "".join(getattr(part, "text", str(part)) for part in msg.content) if isinstance(msg.content, list) else msg.content
        data = json.loads(content_str)

        new_title = (data.get("title") or title).strip()
        new_summary = (data.get("summary") or "").strip()
        new_body = (data.get("content") or content).strip()
        new_tags = data.get("tags") or []

        if not new_title.startswith("[ë°”ì´ë‚¸ìŠ¤]"):
            new_title = f"[ë°”ì´ë‚¸ìŠ¤] {new_title}"

        if not isinstance(new_tags, list):
            new_tags = []
        new_tags = [str(t).strip() for t in new_tags if str(t).strip()][:8]

        if not new_summary:
            new_summary = (new_body[:120] + "...") if len(new_body) > 120 else new_body

        return new_title, new_summary, new_body, new_tags

    except Exception as e:
        print("[WARN] OpenAI ì¬ì‘ì„± ì‹¤íŒ¨:", e)
        fallback_title = f"[ë°”ì´ë‚¸ìŠ¤] {title}" if not title.startswith("[ë°”ì´ë‚¸ìŠ¤]") else title
        fallback_summary = (content[:120] + "...") if len(content) > 120 else content
        return fallback_title, fallback_summary, content, []




def fetch_wp_posts(
    max_posts: int = MAX_POSTS, per_page: int = PER_PAGE
) -> list[dict]:
    """
    WP REST APIì—ì„œ posts JSONì„ ìµœëŒ€ max_postsê¹Œì§€ ê°€ì ¸ì˜¨ë‹¤.
    _embed=1ì„ ë¶™ì—¬ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ ì •ë³´ê¹Œì§€ ê°€ì ¸ì˜¨ë‹¤.
    """
    collected: list[dict] = []
    page = 1

    if not WP_API_BASE:
        raise RuntimeError("WP_API_BASE í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    while len(collected) < max_posts:
        params = {"per_page": per_page, "page": page, "_embed": "1"}
        print(f"[INFO] WP posts ìš”ì²­: page={page}, per_page={per_page}")
        resp = requests.get(WP_API_BASE, params=params, timeout=10)

        if resp.status_code != 200:
            print(f"[WARN] WP API ìš”ì²­ ì‹¤íŒ¨ status={resp.status_code}")
            break

        items = resp.json()
        if not items:
            print("[INFO] ë” ì´ìƒ ê°€ì ¸ì˜¬ í¬ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            break

        collected.extend(items)
        if len(items) < per_page:
            break

        page += 1

    return collected[:max_posts]


def main():
    print(f"[INFO] WP JSONì—ì„œ í¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {WP_API_BASE}")
    posts = fetch_wp_posts(MAX_POSTS, PER_PAGE)
    print(f"[INFO] ì´ ê°€ì ¸ì˜¨ í¬ìŠ¤íŠ¸ ìˆ˜: {len(posts)}")

    for post in posts:
        # ì› ì œëª©
        raw_title = post.get("title", {}).get("rendered", "") or "ì œëª© ì—†ìŒ"
        orig_title = (
            BeautifulSoup(raw_title, "html.parser").get_text().strip()
        )

        # ë§í¬ (ì´ë¯¸ì§€ ì ˆëŒ€ ê²½ë¡œ ê³„ì‚°ì—ë§Œ ì‚¬ìš©)
        link = post.get("link", "").strip()

        # ë‚ ì§œ
        raw_date = post.get("date") or post.get("date_gmt") or ""
        try:
            dt = datetime.fromisoformat(raw_date.replace("Z", ""))
        except Exception:
            dt = datetime.now()

        date_str = dt.strftime("%Y-%m-%d")
        year = dt.strftime("%Y")
        month = dt.strftime("%m")

        # slug (ì›ë˜ ì œëª© ê¸°ì¤€ìœ¼ë¡œ ë§Œë“œëŠ” ê²Œ ì•ˆì „)
        slug_base = slugify(orig_title) or "untitled"
        slug = f"{date_str}-{slug_base}"

        # ê²½ë¡œ
        content_dir = os.path.join(CONTENT_BASE, year, month)
        ensure_dir(content_dir)
        md_path = os.path.join(content_dir, f"{slug}.md")

        if os.path.exists(md_path):
            print(f"[SKIP] ì´ë¯¸ ì¡´ì¬: {md_path}")
            continue

        # ë³¸ë¬¸ HTML
        raw_content_html = (
            post.get("content", {}).get("rendered", "")
            or post.get("excerpt", {}).get("rendered", "")
            or ""
        )

        body_text_raw = clean_html_to_markdown(raw_content_html)

        # ğŸ”¹ OpenAIë¡œ ì œëª©+ë³¸ë¬¸ ì¬ì‘ì„± (í•œêµ­ì–´ ê¸°ì‚¬ + ìš”ì•½ + íƒœê·¸)
        new_title, new_summary, new_body, new_tags = rewrite_with_openai(
            orig_title, body_text_raw
        )
        title = new_title
        summary_text = new_summary
        body_text = new_body
        tags = new_tags
        print(f"[AI] ì œëª© ì¬ì‘ì„±: '{orig_title}'  â†’  '{title}'")

        # ğŸ”¹ ëŒ€í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ (REST API + fallback)
        img_url = extract_featured_image_from_post(
            post, raw_content_html, base_url=link
        )
        featured_image = ""

        if img_url:
            try:
                parsed = urlparse(img_url)
                ext = os.path.splitext(parsed.path)[1]
                if ext.lower() not in [".jpg", ".jpeg", ".png", ".webp"]:
                    ext = ".jpg"

                img_dir = os.path.join(IMAGE_BASE, year, month)
                ensure_dir(img_dir)
                img_filename = slug + ext
                img_path = os.path.join(img_dir, img_filename)

                print(f"[IMG] ë‹¤ìš´ë¡œë“œ: {img_url} -> {img_path}")

                headers = {
                    "User-Agent": (
                        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    )
                }
                r = requests.get(img_url, headers=headers, timeout=10)
                print(f"[IMG] status={r.status_code}")

                if r.status_code == 200:
                    with open(img_path, "wb") as f:
                        f.write(r.content)
                    # XMag ë¦¬ìŠ¤íŠ¸ìš©: thumbnail: "images/news/YYYY/MM/íŒŒì¼ëª…"
                    featured_image = f"news/{year}/{month}/{img_filename}"
                else:
                    print(
                        f"[WARN] ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ status={r.status_code}"
                    )
            except Exception as e:
                print(f"[WARN] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # ===== front matter ì‘ì„± =====
        safe_title = title.replace('"', '\\"')
        safe_summary = (summary_text or "").replace('"', '\\"')

        front_matter = "---\n"
        front_matter += f'title: "{safe_title}"\n'
        front_matter += f"date: {date_str}{TIME_SUFFIX}\n"
        front_matter += f"lastmod: {date_str}{TIME_SUFFIX}\n"
        front_matter += "draft: false\n"
        front_matter += f'categories: ["{DEFAULT_CATEGORY}"]\n'

        # ğŸ”¹ íƒœê·¸ ì±„ìš°ê¸° (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
        front_matter += "tags:\n"
        for t in (tags or []):
            safe_tag = str(t).replace('"', '\\"')
            front_matter += f'  - "{safe_tag}"\n'

        front_matter += f'summary: "{safe_summary}"\n'

        if featured_image:
            # XMag list.htmlì—ì„œ .Params.thumbnail ì„ ë³´ê³  ìˆìœ¼ë¯€ë¡œ thumbnail ì‚¬ìš©
            front_matter += f'thumbnail: "{featured_image}"\n'

        front_matter += "---\n\n"

        full_content = front_matter + body_text + "\n"

        with open(md_path, "w", encoding="utf-8") as f:
            f.write(full_content)

        print(f"[OK] ìƒì„±: {md_path}")

    print("[DONE] WP JSON â†’ Hugo ë³€í™˜ ì™„ë£Œ")


if __name__ == "__main__":
    main()
